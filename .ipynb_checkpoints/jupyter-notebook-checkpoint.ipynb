{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Play Chess Using Deep Reinforcement Learning\n",
    "\n",
    "### Abstract\n",
    "In this project, we aim to develop a deep reinforcement learning agent using the deep Q-network (DQN) algorithm. The objective is to train the agent to proficiently play the complex game of chess. To accomplish this, we will define states, rewards, actions, and quality metrics to guide the training process. Additionally, we will explore and analyze other prominent methods used in the field of chess AI. By reviewing these alternative techniques, we aim to gain a comprehensive understanding of the diverse range of solution approaches available.\n",
    "\n",
    "### Approach:\n",
    "In order to train our chess-playing agent effectively, we will focus on representing the following four main aspects:\n",
    "\n",
    "1. **State (S)**:\n",
    "To represent the current state of the chess game, we will use a numpy array of size 64. Each index of the array corresponds to a specific position on the chessboard. For example, the case B3 on the board would be represented by state[10]. Additionally, we will numerize the chess pieces based on their classical values, which are relative to the player's side. This numerical representation will help evaluate the board using a heuristic-based policy. The values attributed to each piece is as follows:  \n",
    ">   'Pawn': 10,  \n",
    "    'Enemy Pawn': -10,  \n",
    "    'Queen': 90,  \n",
    "    'Enemy Queen': -90,  \n",
    "    'Knight': 30,  \n",
    "    'Enemy Knight': -30,  \n",
    "    'Rook': 50,  \n",
    "    'Enemy Rook': -50,  \n",
    "    'Bishop': 30,  \n",
    "    'Enemy Bishop': -30,  \n",
    "    'King': 900,  \n",
    "    'Enemy King': -900,  \n",
    "    'None': 0\n",
    "\n",
    "2. **Reward (R)**:\n",
    "The reward is a crucial component in reinforcement learning. In the case of chess, defining a direct reward is challenging due to the long-term strategies involved. To address this, we will utilize the Monte-Carlo method, which considers future rewards. The reward at each step (Reward_i) will be determined by multiplying a time factor (Gamma) with the winner's reward. Similarly, a malus (Malus_i) will be calculated for the loser by multiplying Gamma with the loser's malus (a negative value). This stochastic reward process helps guide the learning process.\n",
    "> Reward_i := (Gamma*i) * Winner_Reward  \n",
    "  Malus_i  := (Gamma*i) * Loser_Malus (negative value)\n",
    "\n",
    "3. **Action (A)**:\n",
    "The action component represents the set of possible moves that a player can make from the current state. As chess follows a Markov Decision Process (MDP), the choice of moves is partly random and partly guided by a decision-making algorithm. The actions available at each state will be determined based on the rules of the game.\n",
    "\n",
    "4. **Quality (Q(Si, Ai))**:\n",
    "The quality factor represents the evaluation of the quality for a given action (Ai) in a specific state (Si). Initially, all quality values will be initialized with zeros. It can be represented as a matrix where the columns correspond to actions and the rows correspond to states. After each batch of games, the quality values will be updated with the corresponding rewards/malus, reflecting the agent's learning.\n",
    "By considering and representing these four main aspects, we can construct a framework for training our chess-playing agent using reinforcement learning techniques.\n",
    "\n",
    "### Random Factor:\n",
    "To ensure exploration and prevent the agent from getting stuck in a suboptimal strategy, we incorporate a random factor known as epsilon. Initially, when training begins, all Q-values are initialized to zero. During the early stages, the agent randomly selects moves to explore the various possibilities. However, as the training progresses, the agent tends to favor the moves with higher Q-values, limiting its exploration. To address this, we introduce epsilon, which determines the probability of selecting a random move (exploration) versus choosing the move with the highest Q-value (exploitation). The epsilon value is decremented at the end of each game using a decremental_epsilon approach. As more games are played, the agent gradually reduces the randomness and relies more on the learned Q-values to make decisions.\n",
    "> \tepsilon := epsilon - decremental_epsilon\n",
    "\n",
    "### Deep-Q-Network (DQN):\n",
    "Considering the complexity of chess and the dynamic nature of both the input and target values, we employ a Deep Q-Network (DQN) architecture. The objective is to associate each (state_i, action_i) pair with a corresponding Q-value. Our DQN consists of four dense layers. The input shape of the network is (65,), where the first 64 dimensions represent the current state of the chessboard, and the last dimension corresponds to the chosen move. During the decision-making process, when the agent randomly selects a Q-dependent move:\n",
    "1. The model predicts the Q-values for each possible move from the given state.\n",
    "2. The agent selects the move that maximizes the predicted Q-value, indicating the most promising action to take.\n",
    "3. At the end of each game, the DQN is trained using a new batch of data. This training process involves updating the network's weights based on the observed state, the predicted Q-values for the state along with the available moves, and the obtained reward during the game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code\n",
    "First we import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import random\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "import pickle \n",
    "from keras.models import Sequential      # One layer after the other\n",
    "from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network. Input is two consecutive game states, output is Q-values of the possible moves.\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(65,) , kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(18, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(18, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(18, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(18, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='relu'))    # Same number of outputs as possible actions\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "state_board=np.zeros((1,65)) # The array representing the board of 8*8 and also a selected move from the possible ones\n",
    "\n",
    "# Value of every piece\n",
    "switch={\n",
    "            'p':10,\n",
    "            'P':-10,\n",
    "            'q':90,\n",
    "            'Q':-90,\n",
    "            'n':30,\n",
    "            'N':-30,\n",
    "            'r':50,\n",
    "            'R':-50,\n",
    "            'b':30,\n",
    "            'B':-30,\n",
    "            'k':900,\n",
    "            'K':-900,\n",
    "            'None':0\n",
    "        }\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--number_of_games',type=float, default=100)\n",
    "parser.add_argument('--winner_reward',type=float, default=1)\n",
    "parser.add_argument('--loser_malus',type=float, default=-1)\n",
    "parser.add_argument('--epsilon',type=float, default=1)\n",
    "parser.add_argument('--decremental_epsilon',type=float, default=0.0001)\n",
    "parser.add_argument('--gamma',type=float, default=0.05)\n",
    "args = parser.parse_args()\n",
    "arguments = {'training_games': args.number_of_games, 'winner_reward': args.winner_reward,'loser_malus': args.loser_malus, 'epsilon': args.epsilon,'decremental_epsilon': args.decremental_epsilon, 'gamma': args.gamma}\n",
    "general_moves={}\n",
    "\n",
    "steps=1000\n",
    "training_games=int(arguments['training_games']) if (arguments['training_games'] is not None) else 100\n",
    "winner_reward=int(arguments['winner_reward']) if (arguments['winner_reward'] is not None) else 1\n",
    "loser_malus=int(arguments['loser_malus']) if (arguments['loser_malus'] is not None) else -1\n",
    "epsilon = float(arguments['epsilon']) if (arguments['epsilon'] is not None) else 1                            # Probability of doing a random move\n",
    "decremental_epsilon=float(arguments['decremental_epsilon']) if (arguments['decremental_epsilon'] is not None) else 1/training_games    # Each game we play we want to decrease the probability of random move\n",
    "gamma = float(arguments['gamma']) if (arguments['gamma'] is not None) else 0.05                                # Discounted future reward. How much we care about steps further in time\n",
    "\n",
    "print(\"Training the Deep-Q-Network with parameters : \")\n",
    "print(\"Number of training games : \"+str(training_games))\n",
    "print(\"Winner Reward : \"+str(winner_reward))\n",
    "print(\"Loser Malus : \"+str(loser_malus))\n",
    "print(\"Epsilon : \"+str(epsilon))\n",
    "print(\"Decremental Epsilon : \"+str(decremental_epsilon))\n",
    "print(\"Gamma : \"+str(gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the board following the value of each piece\n",
    "def evaluate_board(turn):\n",
    "    l=chess.SQUARES\n",
    "    total=0\n",
    "    if turn:\n",
    "        mult=1\n",
    "    else:\n",
    "        mult=-1\n",
    "    a=0\n",
    "    b=0\n",
    "    for i in l:\n",
    "        total=total+(mult*switch[str(board.piece_at(i))])\n",
    "        state_board[0][a]=mult*switch[str(board.piece_at(i))] # Update the state_board variable used for predictions\n",
    "        a+=1\n",
    "    return total\n",
    " \n",
    "# Give the int representation(maping) of the move from the dictionary to give it as input for the deep neural network\n",
    "def get_int(move):  \n",
    "    try:\n",
    "        return general_moves[str(move)]\n",
    "    except:\n",
    "        general_moves[str(move)]=len(general_moves)\n",
    "        return general_moves[str(move)]\n",
    "\n",
    "# the final reward at the end of the game that reward each (state,move) of winner (and decrease the ones of loser).\n",
    "def reward(fen_history, moves, lose_fen, lose_moves):\n",
    "    maxi=len(fen_history)\n",
    "    i=0\n",
    "    inputs=[]\n",
    "    targets=[]\n",
    "    while i<maxi:\n",
    "        gamma=1/len(fen_history)\n",
    "        fen_history[i][0][64]=get_int(moves[i])\n",
    "        inputs.append(fen_history[i][0])\n",
    "        model.train_on_batch(np.array(fen_history[i]),model.predict(np.array(fen_history[i]))+winner_reward*(gamma*i))\n",
    "        i=i+1\n",
    "    maxi=len(lose_fen)\n",
    "    i=0\n",
    "    while i<maxi:\n",
    "        gamma=1/len(lose_fen)\n",
    "        lose_fen[i][0][64]=get_int(lose_moves[i])\n",
    "        model.train_on_batch(np.array(lose_fen[i]),model.predict(np.array(lose_fen[i]))+loser_malus*(gamma*i))\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The main part of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners={}    # Variable for counting number of wins of each player\n",
    "for joum in range(0, steps):\n",
    "    i=0\n",
    "    evaluation_history=[]\n",
    "    all_number_of_moves=[]            \n",
    "    board=chess.Board()\n",
    "    epsilon=1\n",
    "    decremental_epsilon=1/training_games\n",
    "    while i<training_games:\n",
    "        os.system('clear')\n",
    "        print(\"/------------------ Training -----------------/\")\n",
    "        print(\"Step (\"+str(joum)+\"/\"+str(steps)+\")\")\n",
    "        print(\"Game N°\"+str(i))\n",
    "        print(\"WINNERS COUNT : \\n\"+str(winners))\n",
    "        print(\"Number of remaining training games : \"+str(training_games-i))\n",
    "        print(\"Winner Reward : \"+str(winner_reward))\n",
    "        print(\"Loser Malus : \"+str(loser_malus))\n",
    "        print(\"Epsilon : \"+str(epsilon))\n",
    "        print(\"Decremental Epsilon : \"+str(decremental_epsilon))\n",
    "        print(\"Gamma : \"+str(gamma))\n",
    "        fen_history=[]\n",
    "        black_moves=[]\n",
    "        white_moves=[]\n",
    "        black_fen_history=[]\n",
    "        white_fen_history=[]\n",
    "        all_states=[]\n",
    "        all_moves=[]\n",
    "        number_of_moves=0\n",
    "        evaluation_history=[]\n",
    "        while not board.is_game_over():\n",
    "            number_of_moves=number_of_moves+1\n",
    "            if np.random.rand() <= epsilon:\n",
    "                nmov=random.randint(0,board.legal_moves.count())\n",
    "                cnt=0\n",
    "                for k in board.legal_moves:\n",
    "                    if cnt==nmov: \n",
    "                        god_damn_move = str(k)\n",
    "                    cnt+=1\n",
    "                        \n",
    "            else:\n",
    "                evaluate_board(True)\n",
    "                Q={}\n",
    "                for kr in board.legal_moves:\n",
    "                    br=get_int(kr)\n",
    "                    state_board[0][64]=br\n",
    "                    Q[kr]=model.predict(state_board)          # Q-values predictions for every action possible with the actual state\n",
    "                god_damn_move = max(Q.items(), key=operator.itemgetter(1))[0] # Get the move with the highest Q-value\n",
    "            base_evaluation=evaluate_board(board.turn)\n",
    "            fen=str(board.fen())\n",
    "            all_states.append(np.array(state_board,copy=True))\n",
    "            all_moves.append(np.array(god_damn_move,copy=True))\n",
    "            evaluation_history.append(base_evaluation)\n",
    "            if board.turn:\n",
    "                white_moves.append(god_damn_move)\n",
    "                white_fen_history.append(np.array(state_board,copy=True))\n",
    "            else:\n",
    "                black_moves.append(god_damn_move)\n",
    "                black_fen_history.append(np.array(state_board,copy=True))\n",
    "            if board.is_legal(chess.Move.from_uci(str(god_damn_move))):\n",
    "               board.push(chess.Move.from_uci(str(god_damn_move)))\n",
    "            else:\n",
    "                print(\"Move not valid: \",god_damn_move)\n",
    "        all_number_of_moves.append(number_of_moves)\n",
    "        \n",
    "        i=i+1\n",
    "        if board.result()==\"1-0\":\n",
    "            reward(white_fen_history, white_moves, black_fen_history, black_moves)\n",
    "        elif board.result()==\"0-1\":\n",
    "            reward(black_fen_history, black_moves, white_fen_history, white_moves)\n",
    "        try:\n",
    "            winners[str(board.result())]=winners[str(board.result())]+1\n",
    "        except:\n",
    "            winners[str(board.result())]=1\n",
    "        board.reset()\n",
    "        epsilon-=decremental_epsilon \n",
    "\n",
    "\n",
    "print(\"WINNERS COUNT : \\n\"+str(winners))\n",
    "with open('generalized_moves.json', 'w') as fp:   # Save the mapping Move/Index to be used on developement\n",
    "    json.dump(general_moves, fp)\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "graph = tf.compat.v1.get_default_graph()\n",
    "with graph.as_default():\n",
    "    model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "After running 100000 games for almost two days, the following results was attained:\n",
    "\n",
    "\n",
    "### Comparison\n",
    "This rest of this report explores the various methods used in deep reinforcement learning to learn the game of chess.\n",
    "\n",
    "#### Monte Carlo Tree Search\n",
    "In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes, most notably those employed in software that plays board games. It is an effective search algorithm that approximates the value of a position by constructing a search tree through random exploration of promising moves.\n",
    "\n",
    "To create a search tree for a given position, MCTS executes the following algorithm multiple times, referred to as MCTS simulations, which consist of four steps. It is important to note that the third step of the algorithm is also known as simulation and should not be confused with the overall execution of the algorithm. [1]\n",
    "\n",
    "1. Selection\n",
    "Beginning from the root node, a child node is selected based on a formula of choice. Most MCTS implementations employ a variant of the Upper Confidence Bound (UCB) [2] formula. The process continues until an unvisited (= 'expanded') node is reached, which becomes a leaf node. If the root node is a leaf node, the next step is immediately initiated.\n",
    "\n",
    "2. Expansion\n",
    "If the selected leaf node represents a terminal state (the end of the game), the algorithm proceeds to the backpropagation step. Otherwise, a child node is created for every possible action that can be taken from the selected node.\n",
    "\n",
    "3. Simulation / Rollout\n",
    "A random child node that was expanded in the previous step is chosen, and the remainder of the game is simulated from that particular position by making random moves.\n",
    "\n",
    "4. Backpropagation\n",
    "The result of the simulation is propagated up the tree. Each node keeps track of the number of visits it has received and the number of wins it has contributed to.\n",
    "\n",
    "For chess, this algorithm exhibits inefficiency due to the requirement of simulating an entire game in the third step of every simulation. To calculate the value of a single position, hundreds of simulations are needed to obtain a reliable estimation. Consequently, it is crucial to carefully select the formula used in the selection step to achieve a balance between exploration and exploitation [1].\n",
    "\n",
    "![](images/1.png)\n",
    "\n",
    "### Alpha Go\n",
    "AlphaGo, developed by DeepMind Technologies, a subsidiary of Google, emerged in 2014 as a groundbreaking algorithm designed to excel in the game of Go. Prior to its development, existing Go engines could only surpass amateur players. AlphaGo employed a combination of the Monte Carlo Tree Search (MCTS) algorithm and a deep neural network for position evaluation. The construction of AlphaGo [3], [4] involved an initial phase of supervised learning. A neural network was trained using data derived from human games. The network's weights were subsequently transferred to a new reinforcement learning network. This reinforcement learning network was then employed to generate a training set through self-play. During self-play, the network played against itself, recording the current board state, the moves the network considered, and the ultimate winner of the game for each move. The resulting training set was used to train the reinforcement learning network. Additionally, a separate network known as the value network was utilized to estimate the value of a given position.\n",
    "\n",
    "### AlphaGo Zero\n",
    "In order to further advance the capabilities of AlphaGo, DeepMind took the next leap by developing AlphaGo Zero, a version that was trained entirely from scratch. Unlike its predecessor, AlphaGo Zero discarded the use of amateur games for learning [5]. The architecture of AlphaGo Zero differs from AlphaGo in several ways. Instead of employing two separate networks, it utilizes a unified network with two outputs: a policy output and a value output.\n",
    "\n",
    "### AlphaZero\n",
    "AlphaZero represents a broader iteration of the AlphaGo Zero system, designed to excel in various board games, including chess, shogi, and Go [25].\n",
    "#### Neural network input\n",
    "The neural network's input is designed to capture the current state of the game and has a specific structure: [N, N, (M · T + L)]:\n",
    "- N represents the size of the game board (N = 8 in the case of chess).  \n",
    "- M denotes the number of distinct pieces present on the board:  \n",
    "  - In chess, there are two players with six different types of pieces each.  \n",
    "  - Each piece is represented by its own 8x8 board using boolean values.  \n",
    "  - For every square on the board, a value of 1 indicates the presence of the corresponding piece, while 0 denotes its absence.  \n",
    "  - In the case of chess, M=12.    \n",
    "- T signifies the number of previous moves included as part of the input, including the current move. \n",
    "  - AlphaZero employed T = 8 for chess, shogi, and Go.  \n",
    "  - This enables the network to leverage a certain historical context for learning.  \n",
    "- L encompasses a set of game-specific rules:  \n",
    "  - In chess, L equals 7.  \n",
    "  - 1 plane indicating whose turn it is.  \n",
    "  - 1 plane signifies the total number of moves played thus far.  \n",
    "  - 4 planes account for the legality of castling (both players can castle kingside or queenside under specific conditions).  \n",
    "  - 1 plane represents the repetition count, wherein 3 repetitions in chess result in a draw.  \n",
    "\n",
    "Thus, the input has the shape [8, 8, (12 · 8 + 7)].  \n",
    "#### Neural network output\n",
    "The neural network features two distinct outputs: a policy head and a value head. The policy head generates a probability distribution encompassing the possible actions, while the value head provides an evaluation of the current position.\n",
    "\n",
    "The value head simply outputs a single floating-point value within the range of -1 to 1. In contrast, the policy head is more intricate. It produces a vector of probabilities, with each element corresponding to a potential action in the specific game. In the case of chess, there are 73 different types of actions considered:\n",
    "- 56 potential moves akin to those of a queen: eight directions to move the piece, covering a distance between 1 and 7 squares.\n",
    "- 8 feasible knight moves.\n",
    "- 9 specialized \"underpromotion\" moves:\n",
    "  - When a pawn is promoted to a queen, it is regarded as a queen-like move.\n",
    "  - if a pawn is promoted to a rook, bishop, or knight, it is considered an underpromotion (3 pieces).\n",
    "  - 3 ways to accomplish promotion: advancing the pawn to the final rank or capturing a piece diagonally and landing on the final rank. \n",
    "  Thus, 3 x 3 = 9 underpromotion possibilities.\n",
    "\n",
    "Each of these 73 actions is represented by an 8x8 plane of floating-point values. For instance, the first plane could signify a queen-like move of shifting a piece one square to the northwest, while the second plane might represent the same type of move but covering a distance of two squares. The individual squares within these planes indicate the starting square of the corresponding piece. Consequently, the policy head output comprises a vector of probabilities with a dimension of 73 x 8 x 8, resulting in a total of 4,672 floating-point values.\n",
    "#### Results\n",
    "Performance of AlphaZero in chess compared with the 2016 TCEC world champion program Stockfish. (Trained for 700,000 steps)\n",
    "\n",
    "![](images/2.png)\n",
    "\n",
    "Elo ratings were computed from games between different players where each player was given 1 s per move. The Elo rating system is a method for calculating the relative skill levels of players in zero-sum games such as chess.\n",
    "\n",
    "### Links to other materials\n",
    "1. Link to YouTube Video: https://youtu.be/7ht-pKO7nGI\n",
    "\n",
    "2. Link to Supplemantary Materials: https://github.com/am-nu-fall-22/emotion-recognition\n",
    "\n",
    "### References\n",
    "[1] “Monte Carlo tree search,” Wikipedia, Available: https://en.wikipedia.org/w/index.php?title=Monte_Carlo_tree_search&oldid=1081107255\n",
    "[2] ML | Monte Carlo Tree Search (MCTS), Available: https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/\n",
    "[3] AlphaGo, Available: https://www.deepmind.com/research/highlighted-research/alphago\n",
    "[4] Mastering the game of Go with Deep Neural Networks & Tree Search. Available: https://www.deepmind.com/publications/mastering-the-game-of-go-with-deep-neural-networks-tree-search\n",
    "[5] Mastering the game of Go without Human Knowledge, Available: https://www.deepmind.com/publications/mastering-the-game-of-go-without-human-knowledge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
